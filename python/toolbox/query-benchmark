#!/usr/bin/env python

import sys
import subprocess
import time
import datetime

import logging
import itertools

import pyarrow
import pandas
import numpy

from skyhookdm_singlecell import skyhook
from skyhookdm_singlecell.util import ArgparseBuilder
from skyhookdm_singlecell.parsers import QueryParser
from skyhookdm_singlecell.dataformats import arrow_table_from_binary, arrow_batches_from_binary


# ------------------------------
# Parse command-line arguments first
parsed_args, parsed_extra_args = (
    ArgparseBuilder.with_description("Thin wrapper around SkyhookDM's run-query binary")
                   .add_skyhook_query_bin_args(required=False)
                   .add_skyhook_obj_slice_args(required=False)
                   .add_ceph_pool_arg(required=True)
                   .add_skyhook_table_arg(required=False)
                   .add_metadata_target_arg(
                         required=False
                        ,help_str='Metadata to query *instead of* table data: <cells | genes>'
                    )
                   .add_query_arg(required=False)
                   .parse_args()
)


# Bootstrap logger
logger = logging.getLogger('toolbox.run-query')
logger.addHandler(logging.StreamHandler(sys.stdout))
logger.setLevel(logging.INFO)


def construct_query_sequence():
    """
    Convenience function to construct the run-query command string from command-line arguments and
    execute the skyhook run-query binary with these arguments.

    This is a generator and yields, for each table (relation), the skyhook results as a sequence of
    binary size (8 bytes) and binary data (arrow format).
    """

    # parse query string if provided
    query_components = None
    if parsed_args.query_str is not None:
        query_components = QueryParser.parse(parsed_args.query_str)

    # variables for coalescing query string into other arguments
    projection_attrs  = []
    project_cell_meta = False
    project_gene_meta = False

    # process attributes specified in select clause
    if query_components.get('projection'):
        for project_attr in query_components.get('projection'):
            if project_attr not in ('cell-metadata', 'gene-metadata'):
                projection_attrs.append(project_attr)

            elif project_attr == 'cell-metadata':
                project_cell_meta = True

            elif project_attr == 'gene-metadata':
                project_gene_meta = True

    # process attributes specified in from clause
    if parsed_args.skyhook_table:
        relation_list = [parsed_args.skyhook_table]

    elif query_components.get('relations'):
        relation_list = query_components.get('relations')

    # yield a sequence of argument lists, where each argument list is for a query
    for relation in relation_list:
        # construct query args from command-line args
        command_args = [
            parsed_args.path_to_query_bin,

            '--start-obj'    , parsed_args.start_obj             ,
            '--num-objs'     , parsed_args.num_objs              ,
            '--oid-prefix'   , 'public'                          ,
            '--pool'         , parsed_args.ceph_pool             ,
            '--table-name'   , relation                          ,
            '--result-format', str(skyhook.FormatTypes.SFT_ARROW),
            '--output-format', 'SFT_PYARROW_BINARY'              ,
        ]

        # add arguments that may have been specified
        if projection_attrs:
            command_args += ['--project', ','.join(projection_attrs)]

        if parsed_args.metadata_target == 'cells' or project_cell_meta:
            command_args += ['--cell-metadata']

        elif parsed_args.metadata_target == 'genes' or project_gene_meta:
            command_args += ['--gene-metadata']

        yield command_args


def execute_query(query_args):
    # execute run-query binary and return binary response
    # print(f'Executing command: "{query_args}"')

    cmd_completion = subprocess.run(query_args, check=True, stdout=subprocess.PIPE)
    return cmd_completion.stdout


def arrow_table_from_query_response(binary_response, size_width=8, show_progress=False):
    """
    Convenience function to call the run-query binary and handle output.
    """

    byte_ndx_blob      = 0
    schema_cumulative  = None
    batches_cumulative = None

    if show_progress:
        query_ndx       = 0
        conversion_time = 0
        sys.stderr.write('Parsing query response into arrow table...\n')

    # iterate over each partition in the output
    while byte_ndx_blob < len(binary_response):
        # first, peel the blob size
        byte_ndx_size = byte_ndx_blob + size_width
        data_size     = int.from_bytes(
            binary_response[byte_ndx_blob:byte_ndx_size],
            byteorder='little'
        )

        # then, parse the binary into arrow record batches and a schema
        byte_ndx_blob_end = byte_ndx_size + data_size

        # ------------------------------
        conversion_start = time.time()

        partition_schema, partition_batches = arrow_batches_from_binary(
            binary_response[byte_ndx_size:byte_ndx_blob_end]
        )

        # set our initial accumulators
        if batches_cumulative is None:
            batches_cumulative = partition_batches
            schema_cumulative  = partition_schema

        # otherwise, accumulate
        else:
            batches_cumulative.extend(partition_batches)

            for partition_field in partition_schema:
                schema_cumulative = schema_cumulative.append(partition_field)

        conversion_end = time.time()
        # ------------------------------

        if show_progress:
            query_ndx       += 1
            conversion_size  = byte_ndx_blob_end - byte_ndx_blob
            conversion_time += conversion_end - conversion_start
            current_time     = datetime.datetime.now().strftime('%H:%M:%S')
            sys.stderr.write(
                f'\r[{current_time}] Query Progress: {query_ndx:04d} ({conversion_time:.04f})'
            )

            sys.stdout.write(f'\tConversion size (bytes)  : {conversion_size}\n')
            sys.stdout.write(f'\tConversion time (seconds): {conversion_time:.04f}\n')

        # move our index in the output forward (aka look at next partition)
        byte_ndx_blob = byte_ndx_blob_end

    if show_progress:
        sys.stderr.write('\n')

    # return the overall, coalesced arrow table
    return pyarrow.Table.from_arrays(
        list(itertools.chain.from_iterable([
            partition_batch.columns
            for partition_batch in batches_cumulative
        ])),
        schema=schema_cumulative
    )


def dataframe_from_query_response(binary_response, size_width=8,
                                  show_progress=False, sparse=False):
    """
    Convenience function to call the run-query binary and handle output.
    """

    byte_ndx_blob        = 0
    partition_dataframes = []

    if show_progress:
        query_ndx       = 0
        conversion_time = 0
        sys.stderr.write('Parsing query response into pandas dataframe...\n')

    # iterate over each partition in the output
    while byte_ndx_blob < len(binary_response):
        # first, peel the blob size
        byte_ndx_size = byte_ndx_blob + size_width
        data_size     = int.from_bytes(
            binary_response[byte_ndx_blob:byte_ndx_size],
            byteorder='little'
        )

        # then, parse the binary into arrow record batches and a schema
        byte_ndx_blob_end = byte_ndx_size + data_size
        #result_table = arrow_table_from_binary(binary_response[byte_ndx_size:byte_ndx_blob_end])
        partition_schema, partition_batches = arrow_batches_from_binary(
            binary_response[byte_ndx_size:byte_ndx_blob_end]
        )

        # dtype hardcoded for now
        if sparse:
            conversion_start = time.time()
            partition_dataframes.append(pandas.DataFrame(
                (
                    pandas.arrays.SparseArray(partition_col, fill_value=0, dtype=numpy.uint16)
                    for partition_col in partition_batches[0].columns
                )
            ))
            conversion_end = time.time()

        else:
            conversion_start = time.time()
            partition_dataframes.append(pandas.DataFrame(partition_batches[0].to_pandas()))
            conversion_end = time.time()

        if show_progress:
            query_ndx       += 1
            conversion_size  = byte_ndx_blob_end - byte_ndx_blob
            conversion_time += conversion_end - conversion_start
            current_time     = datetime.datetime.now().strftime('%H:%M:%S')
            sys.stderr.write(
                f'\r[{current_time}] Query Progress: {query_ndx:04d} ({conversion_time:.04f})'
            )

            sys.stdout.write(f'\tConversion size (bytes)  : {conversion_size}\n')
            sys.stdout.write(f'\tConversion time (seconds): {conversion_time:.04f}\n')

        # move our index in the output forward (aka look at next partition)
        byte_ndx_blob = byte_ndx_blob_end

    if show_progress:
        sys.stderr.write('\n')

    # return the overall, coalesced arrow table
    return pandas.concat(partition_dataframes)


def query_skyhook_arrow_table(show_progress=False):
    query_sequence = construct_query_sequence()

    query_time, arrow_time, pandas_time = 0, 0, 0
    count_bytes_received = 0

    if show_progress:
        sys.stderr.write('Showing query progress: <partitions queried> (pct of time querying)\n')

    for relational_ndx, query_args in enumerate(query_sequence):
        query_start    = time.time()
        query_response = execute_query(query_args)
        query_end      = time.time()

        count_bytes_received += len(query_response)

        arrow_start  = time.time()
        result_table = arrow_table_from_query_response(query_response)
        arrow_end    = time.time()

        pandas_start = time.time()
        result_table.to_pandas()
        pandas_end   = time.time()

        query_time  += query_end - query_start
        arrow_time  += arrow_end  - arrow_start
        pandas_time += pandas_end - pandas_start

        if show_progress:
            query_ratio    = query_time / (query_time + arrow_time + pandas_time)
            current_time   = datetime.datetime.now().strftime('%H:%M:%S')
            sys.stderr.write(
                f'\r[{current_time}] Relations queried: {relational_ndx:04d} ({query_ratio:.04f})'
            )


    if show_progress:
        sys.stderr.write('\n')

    print(f'\tQuery  time            (seconds): {query_time:.04f}')
    print(f'\tArrow  conversion time (seconds): {arrow_time:.04f}')
    print(f'\tPandas conversion time (seconds): {pandas_time:.04f}')
    print(f'\tTotal  time            (seconds): {query_time + arrow_time + pandas_time:.04f}')
    print(f'\tTotal data received      (bytes): {count_bytes_received}')


def query_skyhook_dataframe(show_progress=False, sparse=False):
    query_sequence = construct_query_sequence()

    query_time, parse_time = 0, 0
    count_bytes_received   = 0

    if show_progress:
        sys.stderr.write('Showing query progress: <partitions queried> (pct of time querying)\n')

    for relational_ndx, query_args in enumerate(query_sequence):
        query_start    = time.time()
        query_response = execute_query(query_args)
        query_end      = time.time()

        count_bytes_received += len(query_response)

        parse_start  = time.time()
        result_table = dataframe_from_query_response(
            query_response, show_progress=show_progress, sparse=sparse
        )
        parse_end    = time.time()

        query_time  += query_end - query_start
        parse_time  += parse_end - parse_start

        if show_progress:
            query_ratio    = query_time / (query_time + parse_time)
            current_time   = datetime.datetime.now().strftime('%H:%M:%S')
            sys.stderr.write(
                f'[{current_time}] Relations queried: {relational_ndx:04d} ({query_ratio:.04f})\n'
            )

    if show_progress:
        sys.stderr.write('\n')

    print(f'\tQuery      time   (seconds): {query_time:.04f}')
    print(f'\tConversion time   (seconds): {parse_time:.04f}')
    print(f'\tTotal      time   (seconds): {query_time + parse_time:.04f}')
    print(f'\tTotal data received (bytes): {count_bytes_received}')


# ------------------------------
if __name__ == '__main__':
    #query_skyhook_arrow_table(show_progress=True)
    query_skyhook_dataframe(show_progress=True, sparse=True)
